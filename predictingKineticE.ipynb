{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPXlNUg2yJ+xdj+xmtCRCy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.preprocessing import StandardScaler\n","from torchvision import datasets, transforms\n","\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import tqdm\n","\n","# N, D_in, H, D_out = 64, 1000, 100, 10\n","N, D_in, D_out = 100000, 2, 1\n","\n","# 입력과 출력 위한 랜덤 텐서\n","X = []\n","y = []\n","for j in range(N):\n","    X.append([])\n","    #y.append([])\n","    for i in range(D_out):\n","        X[-1].append( np.random.uniform(low=0.0, high=1000.0, size=None) )\n","        X[-1].append( np.random.uniform(low=-1000.0, high=1000.0, size=None) )\n","        y.append( (X[-1][-1]**2) / (2*X[-1][-2])  )\n","\n","\n","X = torch.Tensor(X)\n","y = torch.Tensor(y)\n","\n","# nn package를 이용하여 여러 층으로 정의된 모델 생성\n","# nn.Sequential은 다른 모듈을 담을 수 있는 모듈이며 담겨진 모듈은 순서대로 연결\n","# Linear 모듈은 곧 Affine 모듈\n","\n","# Read data\n","# data = fetch_california_housing()\n","# X, y = data.data, data.target\n","\n","# train-test split for model evaluation\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n","\n","\n"],"metadata":{"id":"K77eFeNvxqcL","executionInfo":{"status":"ok","timestamp":1690806167742,"user_tz":-540,"elapsed":1173,"user":{"displayName":"김통일(이과대학 물리학)","userId":"05565847826958534561"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oy7OKoiU7niw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#default\n","\n","\"\"\"\n","history\n","model = nn.Sequential(\n","    nn.Linear(2, 6),\n","    nn.ReLU(),\n","    nn.Linear(6, 3),\n","    nn.ReLU(),\n","    nn.Linear(3, 2),\n","    nn.ReLU(),\n","    nn.Linear(2, 1)\n",")\n","#optimizer = optim.Adam(model.parameters(), lr=0.0001) # 10 loss:  47942025216.0\n","#optimizer = optim.Adam(model.parameters(), lr=0.001) # 10 loss:   79399010304.0\n","#optimizer = optim.Adam(model.parameters(), lr=0.00001) # 10 loss:439046144000.0\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)    #10 loss: 45356871680.0\n","\n","\"\"\"\n","\n","# Convert to 2D PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n","# Define the model\n","model = nn.Sequential(\n","    nn.Linear(2, 6),\n","    nn.ReLU(),\n","    nn.Linear(6, 3),\n","    nn.ReLU(),\n","    nn.Linear(3, 2),\n","    nn.ReLU(),\n","    nn.Linear(2, 1)\n",")\n","\n","# loss function and optimizer\n","loss_fn = nn.MSELoss()  # mean square error\n","#optimizer = optim.Adam(model.parameters(), lr=0.0001) # 10 loss:  47942025216.0\n","#optimizer = optim.Adam(model.parameters(), lr=0.001) # 10 loss:   79399010304.0\n","#optimizer = optim.Adam(model.parameters(), lr=0.00001) # 10 loss:439046144000.0\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)    #10 loss: 45356871680.0\n","\n","n_epochs = 100   # number of epochs to run\n","batch_size = 10  # size of each batch\n","batch_start = torch.arange(0, len(X_train), batch_size)\n","\n","# Hold the best model\n","best_mse = np.inf   # init to infinity\n","best_weights = None\n","history = []\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n","        bar.set_description(f\"Epoch {epoch}\")\n","        for start in bar:\n","            # take a batch\n","            X_batch = X_train[start:start+batch_size]\n","            y_batch = y_train[start:start+batch_size]\n","            # forward pass\n","            y_pred = model(X_batch)\n","            loss = loss_fn(y_pred/y_batch + y_batch/y_pred, torch.ones(len(y_pred))*2 )\n","            # backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","            # update weights\n","            optimizer.step()\n","            # print progress\n","            bar.set_postfix(mse=float(loss))\n","    # evaluate accuracy at end of each epoch\n","    model.eval()\n","    y_pred = model(X_test)\n","    mse = loss_fn(y_pred/y_test + y_test/y_pred,  torch.ones(len(y_pred))*2)\n","    mse = float(mse)\n","    history.append(mse)\n","    if mse < best_mse:\n","        best_mse = mse\n","        best_weights = copy.deepcopy(model.state_dict())\n","    print(epoch, 'loss:', mse)\n","\n","# restore model and return best accuracy\n","model.load_state_dict(best_weights)\n","print(\"MSE: %.2f\" % best_mse)\n","print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n","plt.plot(history)\n","plt.show()"],"metadata":{"id":"KX3k7kqt45E0","colab":{"base_uri":"https://localhost:8080/","height":806},"executionInfo":{"status":"error","timestamp":1690807078096,"user_tz":-540,"elapsed":155451,"user":{"displayName":"김통일(이과대학 물리학)","userId":"05565847826958534561"}},"outputId":"1cbd7965-769e-437b-9fa3-ae227b0c303e"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-66-6251569b3416>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_train = torch.tensor(X_train, dtype=torch.float32)\n","<ipython-input-66-6251569b3416>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n","<ipython-input-66-6251569b3416>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_test = torch.tensor(X_test, dtype=torch.float32)\n","<ipython-input-66-6251569b3416>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n"]},{"output_type":"stream","name":"stdout","text":["0 loss: 33999474688.0\n","1 loss: 35279228928.0\n","2 loss: 36532441088.0\n","3 loss: 37756350464.0\n","4 loss: 38948450304.0\n","5 loss: 40106991616.0\n","6 loss: 41230303232.0\n","7 loss: 42317606912.0\n","8 loss: 43368144896.0\n","9 loss: 44381331456.0\n","10 loss: 45356871680.0\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-6251569b3416>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"1Y-629XH5qMQ"},"execution_count":null,"outputs":[]}]}